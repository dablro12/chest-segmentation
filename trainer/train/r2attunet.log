2024-05-12 23:39:20.515845: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 23:39:20.517350: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-12 23:39:20.591075: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-12 23:39:21.431295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
wandb: Currently logged in as: dablro1232. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/eiden/eiden/chest-segmentation/src/trainer/train/wandb/run-20240512_233926-s5u4oen3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-wildflower-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dablro1232/Chest-segmentation
wandb: üöÄ View run at https://wandb.ai/dablro1232/Chest-segmentation/runs/s5u4oen3
/home/eiden/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
CUDA Status : cuda
Model: R2AttU_Net loaded successfully!! | pretrained : False
  0%|          | 0/1000 [00:00<?, ?it/s]  0%|          | 0/1000 [00:07<?, ?it/s]
wandb: - 0.003 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.020 MB uploadedwandb: üöÄ View run eternal-wildflower-24 at: https://wandb.ai/dablro1232/Chest-segmentation/runs/s5u4oen3
wandb: Ô∏è‚ö° View job at https://wandb.ai/dablro1232/Chest-segmentation/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE3MzMwNzAxNQ==/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240512_233926-s5u4oen3/logs
Traceback (most recent call last):
  File "./run.py", line 12, in <module>
    main()
  File "./run.py", line 9, in main
    trainer.fit()
  File "/home/eiden/eiden/chest-segmentation/src/trainer/train/train.py", line 147, in fit
    outputs = self.model(images)
  File "/home/eiden/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/eiden/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/eiden/eiden/chest-segmentation/src/trainer/train/../model/models.py", line 216, in forward
    x1 = self.Att2(g=d2,x=x1)
  File "/home/eiden/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/eiden/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/eiden/eiden/chest-segmentation/src/trainer/train/../model/modules/__init__.py", line 66, in forward
    return x*psi
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 32.06 MiB is free. Process 837080 has 3.20 GiB memory in use. Process 839305 has 2.11 GiB memory in use. Process 840111 has 1.87 GiB memory in use. Process 841116 has 2.00 GiB memory in use. Process 842100 has 3.89 GiB memory in use. Including non-PyTorch memory, this process has 10.26 GiB memory in use. Of the allocated memory 9.68 GiB is allocated by PyTorch, and 134.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
